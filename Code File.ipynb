{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2221c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: catboost in /venv/main/lib/python3.12/site-packages (1.2.8)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: scikit-learn in /venv/main/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: holidays in /venv/main/lib/python3.12/site-packages (0.75)\n",
      "Requirement already satisfied: matplotlib in /venv/main/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /venv/main/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: graphviz in /venv/main/lib/python3.12/site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: scipy in /venv/main/lib/python3.12/site-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in /venv/main/lib/python3.12/site-packages (from catboost) (6.1.2)\n",
      "Requirement already satisfied: six in /venv/main/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /venv/main/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /venv/main/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /venv/main/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /venv/main/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /venv/main/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /venv/main/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /venv/main/lib/python3.12/site-packages (from plotly->catboost) (1.43.1)\n",
      "Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost, lightgbm\n",
      "Successfully installed lightgbm-4.6.0 xgboost-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy catboost xgboost lightgbm scikit-learn tqdm holidays matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc03a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:61: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:61: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_1370/1547576745.py:61: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['src_tier_num'] = df['srcid_tier'].str.extract('(\\d+)').astype(float)\n",
      "/tmp/ipykernel_1370/1547576745.py:62: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['dest_tier_num'] = df['destid_tier'].str.extract('(\\d+)').astype(float)\n",
      "/tmp/ipykernel_1370/1547576745.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_1370/1547576745.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_1370/1547576745.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train[col].fillna(global_avg, inplace=True)\n",
      "/tmp/ipykernel_1370/1547576745.py:121: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test[col].fillna(global_avg, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete! Processed data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import holidays\n",
    "\n",
    "def create_features():\n",
    "    # Load datasets\n",
    "    train = pd.read_csv('train.csv', parse_dates=['doj'])\n",
    "    test = pd.read_csv('test.csv', parse_dates=['doj'])\n",
    "    transactions = pd.read_csv('transactions.csv', parse_dates=['doj', 'doi'])\n",
    "\n",
    "    # 1. Booking Curve Features\n",
    "    booking_features = []\n",
    "    for (doj, srcid, destid), group in transactions.groupby(['doj', 'srcid', 'destid']):\n",
    "        feature_row = {'doj': doj, 'srcid': srcid, 'destid': destid}\n",
    "        for dbd in [15, 20, 25, 30]:\n",
    "            dbd_data = group[group['dbd'] == dbd]\n",
    "            feature_row[f'seats_dbd{dbd}'] = dbd_data['cumsum_seatcount'].iloc[0] if not dbd_data.empty else 0\n",
    "            feature_row[f'searches_dbd{dbd}'] = dbd_data['cumsum_searchcount'].iloc[0] if not dbd_data.empty else 0\n",
    "        booking_features.append(feature_row)\n",
    "    booking_df = pd.DataFrame(booking_features)\n",
    "\n",
    "    # Merge with main data\n",
    "    train = train.merge(booking_df, on=['doj', 'srcid', 'destid'], how='left')\n",
    "    test = test.merge(booking_df, on=['doj', 'srcid', 'destid'], how='left')\n",
    "\n",
    "    # 2. Temporal Features\n",
    "    for df in [train, test]:\n",
    "        df['day_of_week'] = df['doj'].dt.dayofweek\n",
    "        df['month'] = df['doj'].dt.month\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['doy_sin'] = np.sin(2 * np.pi * df['doj'].dt.dayofyear / 365)\n",
    "        df['doy_cos'] = np.cos(2 * np.pi * df['doj'].dt.dayofyear / 365)\n",
    "\n",
    "    # 3. Holiday Features\n",
    "    india_holidays = holidays.India(years=[2023, 2024, 2025])\n",
    "    holiday_dates = [pd.Timestamp(date) for date in india_holidays.keys()]\n",
    "\n",
    "    for df in [train, test]:\n",
    "        df['is_national_holiday'] = df['doj'].dt.date.isin(india_holidays.keys()).astype(int)\n",
    "        df['days_to_next_holiday'] = df['doj'].apply(\n",
    "            lambda x: min([(pd.Timestamp(h) - x).days for h in india_holidays.keys() if pd.Timestamp(h) > x], default=30)\n",
    "        )\n",
    "        df['days_since_last_holiday'] = df['doj'].apply(\n",
    "            lambda x: min([(x - pd.Timestamp(h)).days for h in india_holidays.keys() if pd.Timestamp(h) < x], default=30)\n",
    "        )\n",
    "        df['holiday_proximity'] = 1 / (1 + df[['days_to_next_holiday', 'days_since_last_holiday']].min(axis=1))\n",
    "\n",
    "    # 4. Route Features - FIXED: Encode tier columns\n",
    "    route_meta = transactions.groupby(['srcid', 'destid']).first().reset_index()[\n",
    "        ['srcid', 'destid', 'srcid_region', 'destid_region', 'srcid_tier', 'destid_tier']\n",
    "    ]\n",
    "\n",
    "    # Merge route metadata\n",
    "    train = train.merge(route_meta, on=['srcid', 'destid'], how='left')\n",
    "    test = test.merge(route_meta, on=['srcid', 'destid'], how='left')\n",
    "\n",
    "    # Convert tier columns to numerical categories\n",
    "    for df in [train, test]:\n",
    "        # Extract tier number from strings like \"Tier 1\"\n",
    "        df['src_tier_num'] = df['srcid_tier'].str.extract('(\\d+)').astype(float)\n",
    "        df['dest_tier_num'] = df['destid_tier'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "        df['same_region'] = (df['srcid_region'] == df['destid_region']).astype(int)\n",
    "        df['tier_combination'] = df['src_tier_num'].astype(str) + '_' + df['dest_tier_num'].astype(str)\n",
    "        df['is_metro_route'] = ((df['src_tier_num'] == 1) & (df['dest_tier_num'] == 1)).astype(int)\n",
    "\n",
    "    # 5. Derived Features\n",
    "    for df in [train, test]:\n",
    "        # Booking metrics\n",
    "        df['velocity_30_15'] = (df['seats_dbd15'] - df['seats_dbd30']) / 15\n",
    "        df['velocity_25_15'] = (df['seats_dbd15'] - df['seats_dbd25']) / 10\n",
    "        df['conversion_rate_15'] = df['seats_dbd15'] / (df['searches_dbd15'] + 1)\n",
    "        df['conversion_delta'] = df['conversion_rate_15'] - (df['seats_dbd30'] / (df['searches_dbd30'] + 1))\n",
    "\n",
    "        # Booking stability\n",
    "        seats_cols = ['seats_dbd15', 'seats_dbd20', 'seats_dbd25', 'seats_dbd30']\n",
    "        df['booking_stability'] = df[seats_cols].std(axis=1) / (df[seats_cols].mean(axis=1) + 1e-5)\n",
    "\n",
    "        # Interactions\n",
    "        df['metro_holiday'] = df['is_metro_route'] * df['is_national_holiday']\n",
    "\n",
    "        # Booking curve shape\n",
    "        X = np.array([30, 25, 20, 15])\n",
    "        def curve_slope(row):\n",
    "            y = [row[f'seats_dbd{dbd}'] for dbd in [30, 25, 20, 15]]\n",
    "            return np.polyfit(X, y, 1)[0]\n",
    "        df['booking_slope'] = df.apply(curve_slope, axis=1)\n",
    "\n",
    "    # 6. Historical Features\n",
    "    train_sorted = train.sort_values('doj').copy()\n",
    "    train_sorted['hist_avg'] = train_sorted.groupby(['srcid', 'destid'])['final_seatcount'].transform(\n",
    "        lambda x: x.expanding().mean().shift(1)\n",
    "    )\n",
    "    train_sorted['hist_max'] = train_sorted.groupby(['srcid', 'destid'])['final_seatcount'].transform(\n",
    "        lambda x: x.expanding().max().shift(1)\n",
    "    )\n",
    "\n",
    "    # For test data\n",
    "    full_hist = train_sorted.groupby(['srcid', 'destid']).agg(\n",
    "        hist_avg=('final_seatcount', 'mean'),\n",
    "        hist_max=('final_seatcount', 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    test = test.merge(full_hist, on=['srcid', 'destid'], how='left')\n",
    "    train = train_sorted\n",
    "\n",
    "    # Early fill ratio\n",
    "    train['early_fill_ratio'] = train['seats_dbd15'] / (train['hist_max'] + 1)\n",
    "    test['early_fill_ratio'] = test['seats_dbd15'] / (test['hist_max'] + 1)\n",
    "\n",
    "    # Handle missing values\n",
    "    seat_search_cols = [c for c in train.columns if c.startswith('seats_') or c.startswith('searches_')]\n",
    "    for col in seat_search_cols:\n",
    "        train[col].fillna(0, inplace=True)\n",
    "        test[col].fillna(0, inplace=True)\n",
    "\n",
    "    for col in ['hist_avg', 'hist_max']:\n",
    "        global_avg = train[col].mean()\n",
    "        train[col].fillna(global_avg, inplace=True)\n",
    "        test[col].fillna(global_avg, inplace=True)\n",
    "\n",
    "    # Encode categoricals\n",
    "    cat_cols = ['srcid_region', 'destid_region', 'tier_combination']\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(pd.concat([train[col], test[col]], axis=0))\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "    # Drop original tier columns\n",
    "    for df in [train, test]:\n",
    "        df.drop(['srcid_tier', 'destid_tier'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # Save processed data\n",
    "    train.to_csv('train_processed.csv', index=False)\n",
    "    test.to_csv('test_processed.csv', index=False)\n",
    "\n",
    "    print(\"Feature engineering complete! Processed data saved.\")\n",
    "    return train, test\n",
    "\n",
    "# Run feature engineering\n",
    "train_proc, test_proc = create_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e486487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★ GPU acceleration available: True\n",
      "\n",
      "Starting rolling window (time-series) cross-validation...\n",
      "Fold 1 RMSE: 614.1209\n",
      "Fold 2 RMSE: 715.8667\n",
      "Fold 3 RMSE: 473.0987\n",
      "Fold 4 RMSE: 446.6334\n",
      "Fold 5 RMSE: 588.1078\n",
      "Mean CV RMSE: 567.5655\n",
      "\n",
      "Training final model on full data...\n",
      "★ xgb.csv written ✓\n",
      "★ Params: {'subsample': 1.0, 'reg_lambda': 1, 'reg_alpha': 0.1, 'n_estimators': 249, 'min_child_weight': 3, 'max_depth': 8, 'learning_rate': 0.1, 'gamma': 0.3, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# xgb_fixedparams_with_cv.py\n",
    "# ------------------------------------------------------------------\n",
    "# ⋅ Leak-free date split\n",
    "# ⋅ One-hot for categoricals\n",
    "# ⋅ Uses fixed best parameters from tuning\n",
    "# ⋅ Rolling time-series CV\n",
    "# ⋅ Trains on full training data\n",
    "# ⋅ Generates xgb.csv\n",
    "# ------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# ─── Reproducibility ─────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── GPU Availability Check ──────────────────────────────────────\n",
    "def check_gpu_support():\n",
    "    try:\n",
    "        X = np.array([[0,0],[1,1]], dtype=np.float32)\n",
    "        y = np.array([0,1], dtype=np.float32)\n",
    "        params = {'tree_method': 'gpu_hist', 'gpu_id': 0, 'objective': 'reg:squarederror'}\n",
    "        xgb.train(params, xgb.DMatrix(X, y), num_boost_round=1)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "USE_GPU = check_gpu_support()\n",
    "print(f\"★ GPU acceleration available: {USE_GPU}\")\n",
    "\n",
    "TRAIN_CSV = \"train_processed.csv\"\n",
    "TEST_CSV  = \"test_processed.csv\"\n",
    "\n",
    "# ─── 1. Load & Preprocess Data ───────────────────────────────────\n",
    "df_train = pd.read_csv(TRAIN_CSV, parse_dates=[\"doj\"]).sort_values(\"doj\")\n",
    "df_test  = pd.read_csv(TEST_CSV,  parse_dates=[\"doj\"]).sort_values(\"doj\")\n",
    "test_keys = df_test[\"route_key\"].copy()\n",
    "\n",
    "def add_date(df):\n",
    "    out = df.copy()\n",
    "    out[\"day\"]   = out[\"doj\"].dt.day\n",
    "    out[\"month\"] = out[\"doj\"].dt.month\n",
    "    out[\"year\"]  = out[\"doj\"].dt.year\n",
    "    return out.drop(columns=\"doj\")\n",
    "\n",
    "df_train = add_date(df_train)\n",
    "df_test  = add_date(df_test)\n",
    "df_train.drop(columns=\"route_key\", inplace=True, errors=\"ignore\")\n",
    "df_test .drop(columns=\"route_key\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "TARGET  = \"final_seatcount\"\n",
    "FEATS   = [c for c in df_train.columns if c != TARGET]\n",
    "cat_cols = df_train[FEATS].select_dtypes(\"object\").columns.tolist()\n",
    "\n",
    "# ─── 2. One-Hot Encoding (Combined Train+Test) ──────────────────\n",
    "df_comb = pd.concat([df_train[FEATS], df_test[FEATS]], axis=0)\n",
    "df_comb = pd.get_dummies(df_comb, columns=cat_cols, drop_first=True)\n",
    "\n",
    "X_train = df_comb.iloc[:len(df_train)].reset_index(drop=True)\n",
    "X_test  = df_comb.iloc[len(df_train):].reset_index(drop=True)\n",
    "y_train = df_train[TARGET]\n",
    "\n",
    "# ─── 3. Fixed Best Parameters ───────────────────────────────────\n",
    "BEST_PARAMS = {\n",
    "    'subsample': 1.0,\n",
    "    'reg_lambda': 1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'n_estimators': 249,\n",
    "    'min_child_weight': 3,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.1,\n",
    "    'gamma': 0.3,\n",
    "    'colsample_bytree': 0.7\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'eta':               BEST_PARAMS['learning_rate'],\n",
    "    'max_depth':         BEST_PARAMS['max_depth'],\n",
    "    'min_child_weight':  BEST_PARAMS['min_child_weight'],\n",
    "    'subsample':         BEST_PARAMS['subsample'],\n",
    "    'colsample_bytree':  BEST_PARAMS['colsample_bytree'],\n",
    "    'gamma':             BEST_PARAMS['gamma'],\n",
    "    'alpha':             BEST_PARAMS['reg_alpha'],\n",
    "    'lambda':            BEST_PARAMS['reg_lambda'],\n",
    "    'objective':         'reg:squarederror',\n",
    "    'seed':              SEED,\n",
    "}\n",
    "\n",
    "if USE_GPU:\n",
    "    xgb_params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})\n",
    "else:\n",
    "    xgb_params['tree_method'] = 'hist'\n",
    "\n",
    "# ─── 4. Rolling Time-Series CV ───────────────────────────────────\n",
    "print(\"\\nStarting rolling window (time-series) cross-validation...\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    dtrain_cv = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval_cv   = xgb.DMatrix(X_val, label=y_val)\n",
    "    cv_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain_cv,\n",
    "        num_boost_round=BEST_PARAMS['n_estimators']\n",
    "    )\n",
    "    preds = cv_model.predict(dval_cv)\n",
    "    rmse  = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "    rmse_scores.append(rmse)\n",
    "print(f\"Mean CV RMSE: {np.mean(rmse_scores):.4f}\\n\")\n",
    "\n",
    "# ─── 5. Train Final Model & Submit ──────────────────────────────\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test)\n",
    "\n",
    "print(\"Training final model on full data...\")\n",
    "final_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=BEST_PARAMS['n_estimators']\n",
    ")\n",
    "\n",
    "pred_test = final_model.predict(dtest)\n",
    "pd.DataFrame({\n",
    "    \"route_key\": test_keys,\n",
    "    \"final_seatcount\": pred_test\n",
    "}).to_csv(\"xgb.csv\", index=False)\n",
    "\n",
    "print(\"★ xgb.csv written ✓\")\n",
    "print(\"★ Params:\", BEST_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6055a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rolling window (time-series) CV for LGBM…\n",
      "Fold 1 RMSE: 592.7637\n",
      "Fold 2 RMSE: 712.7851\n",
      "Fold 3 RMSE: 470.0131\n",
      "Fold 4 RMSE: 432.7165\n",
      "Fold 5 RMSE: 579.9082\n",
      "Mean CV RMSE: 557.6373\n",
      "\n",
      "Training LGBM on full data…\n",
      "✅ lgbm.csv written ✓\n"
     ]
    }
   ],
   "source": [
    "# lgbm_final.py\n",
    "# ------------------------------------------------------------------\n",
    "# ⋅ Uses provided best parameters\n",
    "# ⋅ Rolling time-series CV\n",
    "# ⋅ Trains on full dataset\n",
    "# ⋅ Predicts test set → lgbm.csv\n",
    "# ------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "\n",
    "# ─── Reproducibility ─────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TRAIN_CSV = \"train_processed.csv\"\n",
    "TEST_CSV  = \"test_processed.csv\"\n",
    "\n",
    "# ─── 1. Load & Prepare Data ─────────────────────────────────────\n",
    "df_train = pd.read_csv(TRAIN_CSV, parse_dates=[\"doj\"]).sort_values(\"doj\")\n",
    "df_test  = pd.read_csv(TEST_CSV,  parse_dates=[\"doj\"]).sort_values(\"doj\")\n",
    "test_keys = df_test[\"route_key\"].copy()\n",
    "\n",
    "def add_date(df):\n",
    "    out = df.copy()\n",
    "    out[\"day\"]   = out[\"doj\"].dt.day\n",
    "    out[\"month\"] = out[\"doj\"].dt.month\n",
    "    out[\"year\"]  = out[\"doj\"].dt.year\n",
    "    return out.drop(columns=\"doj\")\n",
    "\n",
    "df_train = add_date(df_train)\n",
    "df_test  = add_date(df_test)\n",
    "for df in (df_train, df_test):\n",
    "    df.drop(columns=\"route_key\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "TARGET  = \"final_seatcount\"\n",
    "FEATS   = [c for c in df_train.columns if c != TARGET]\n",
    "\n",
    "# ─── 2. One-Hot Encode ───────────────────────────────────────────\n",
    "cat_cols = df_train[FEATS].select_dtypes(\"object\").columns.tolist()\n",
    "df_comb  = pd.concat([df_train[FEATS], df_test[FEATS]], axis=0)\n",
    "df_comb  = pd.get_dummies(df_comb, columns=cat_cols, drop_first=True)\n",
    "\n",
    "X_train = df_comb.iloc[:len(df_train)].reset_index(drop=True)\n",
    "X_test  = df_comb.iloc[len(df_train):].reset_index(drop=True)\n",
    "y_train = df_train[TARGET]\n",
    "\n",
    "# ─── 3. Rolling Time-Series CV ──────────────────────────────────\n",
    "best_params = {\n",
    "    'subsample':         0.8,\n",
    "    'reg_lambda':        0.1,\n",
    "    'reg_alpha':         5,\n",
    "    'num_leaves':        40,\n",
    "    'min_split_gain':    0.3,\n",
    "    'min_child_samples': 35,\n",
    "    'max_depth':         9,\n",
    "    'learning_rate':     0.07,\n",
    "    'colsample_bytree':  0.9,\n",
    "}\n",
    "\n",
    "print(\"Starting rolling window (time-series) CV for LGBM…\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        **best_params,\n",
    "        n_estimators=462,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_val)\n",
    "    rmse  = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "    cv_scores.append(rmse)\n",
    "\n",
    "print(f\"Mean CV RMSE: {np.mean(cv_scores):.4f}\\n\")\n",
    "\n",
    "# ─── 4. Train on Full Data & Submit ─────────────────────────────\n",
    "print(\"Training LGBM on full data…\")\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    **best_params,\n",
    "    n_estimators=462,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "pred_test = final_model.predict(X_test)\n",
    "pd.DataFrame({\n",
    "    \"route_key\": test_keys,\n",
    "    \"final_seatcount\": pred_test\n",
    "}).to_csv(\"lgbm.csv\", index=False)\n",
    "\n",
    "print(\"✅ lgbm.csv written ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc871aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rolling window (time-series) CV for RF…\n",
      "Fold 1 RMSE: 654.2618\n",
      "Fold 2 RMSE: 710.9713\n",
      "Fold 3 RMSE: 495.8746\n",
      "Fold 4 RMSE: 486.8650\n",
      "Fold 5 RMSE: 610.7114\n",
      "Mean CV RMSE: 591.7368\n",
      "\n",
      "Training RF on full data…\n",
      "✅ rf.csv written ✓\n"
     ]
    }
   ],
   "source": [
    "# rf_final.py\n",
    "# ------------------------------------------------------------------\n",
    "# ⋅ RandomForest with fixed best parameters\n",
    "# ⋅ Rolling time-series CV\n",
    "# ⋅ Trains on full dataset\n",
    "# ⋅ Predicts test set → rf.csv\n",
    "# ------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "# ─── Reproducibility ─────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TRAIN_CSV = \"train_processed.csv\"\n",
    "TEST_CSV  = \"test_processed.csv\"\n",
    "\n",
    "# ─── 1. Load & Preprocess ───────────────────────────────────────\n",
    "df_train = pd.read_csv(TRAIN_CSV, parse_dates=[\"doj\"])\n",
    "df_test  = pd.read_csv(TEST_CSV,  parse_dates=[\"doj\"])\n",
    "test_keys = df_test[\"route_key\"].copy()\n",
    "\n",
    "def add_date(df):\n",
    "    out = df.copy()\n",
    "    out[\"day\"]   = out[\"doj\"].dt.day\n",
    "    out[\"month\"] = out[\"doj\"].dt.month\n",
    "    out[\"year\"]  = out[\"doj\"].dt.year\n",
    "    return out.drop(columns=\"doj\")\n",
    "\n",
    "df_train = add_date(df_train)\n",
    "df_test  = add_date(df_test)\n",
    "df_train.drop(columns=\"route_key\", inplace=True, errors=\"ignore\")\n",
    "df_test .drop(columns=\"route_key\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "TARGET = \"final_seatcount\"\n",
    "FEATS  = [c for c in df_train.columns if c != TARGET]\n",
    "\n",
    "# ─── 2. One-Hot Encode ───────────────────────────────────────────\n",
    "cat_cols = df_train[FEATS].select_dtypes(\"object\").columns.tolist()\n",
    "df_comb  = pd.concat([df_train[FEATS], df_test[FEATS]], axis=0)\n",
    "df_comb  = pd.get_dummies(df_comb, columns=cat_cols, drop_first=True)\n",
    "\n",
    "X_train = df_comb.iloc[:len(df_train)].reset_index(drop=True)\n",
    "X_test  = df_comb.iloc[len(df_train):].reset_index(drop=True)\n",
    "y_train = df_train[TARGET]\n",
    "\n",
    "# ─── 3. Rolling Time-Series CV ──────────────────────────────────\n",
    "best_params = {\n",
    "    'n_estimators':       1000,\n",
    "    'min_samples_split':  2,\n",
    "    'min_samples_leaf':   1,\n",
    "    'max_features':       'sqrt',\n",
    "    'max_depth':          30,\n",
    "    'bootstrap':          False\n",
    "}\n",
    "\n",
    "print(\"Starting rolling window (time-series) CV for RF…\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        **best_params,\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_val)\n",
    "    rmse  = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "    cv_scores.append(rmse)\n",
    "\n",
    "print(f\"Mean CV RMSE: {np.mean(cv_scores):.4f}\\n\")\n",
    "\n",
    "# ─── 4. Train on Full Data & Submit ─────────────────────────────\n",
    "print(\"Training RF on full data…\")\n",
    "rf = RandomForestRegressor(\n",
    "    **best_params,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "pred_test = rf.predict(X_test)\n",
    "pd.DataFrame({\n",
    "    \"route_key\": test_keys,\n",
    "    \"final_seatcount\": np.round(pred_test).clip(min=10)\n",
    "}).to_csv(\"rf.csv\", index=False)\n",
    "\n",
    "print(\"✅ rf.csv written ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f160ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv written \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load each submission\n",
    "rf  = pd.read_csv(r\"rf.csv\")\n",
    "xgb = pd.read_csv(r\"xgb.csv\")\n",
    "lgb = pd.read_csv(r\"lgbm.csv\")\n",
    "\n",
    "# 2) Merge them on route_key\n",
    "df = (\n",
    "    rf[['route_key', 'final_seatcount']]\n",
    "    .rename(columns={'final_seatcount':'rf'})\n",
    "    .merge(\n",
    "        xgb[['route_key', 'final_seatcount']].rename(columns={'final_seatcount':'xgb'}),\n",
    "        on='route_key',\n",
    "        how='inner'\n",
    "    )\n",
    "    .merge(\n",
    "        lgb[['route_key', 'final_seatcount']].rename(columns={'final_seatcount':'lgb'}),\n",
    "        on='route_key',\n",
    "        how='inner'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) Compute median across the three predictions\n",
    "df['final_seatcount'] = df[['rf','xgb','lgb']].mean(axis=1)\n",
    "\n",
    "# 4) Clip to at least 10, round to nearest int, and cast\n",
    "df['final_seatcount'] = np.rint(df['final_seatcount'].clip(lower=10)).astype(int)\n",
    "\n",
    "# 5) Save the final ensemble\n",
    "df[['route_key','final_seatcount']].to_csv(r\"submission.csv\", index=False)\n",
    "print(\"submission.csv written \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32bb0f-534f-4312-8332-f3e74cdcf1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51fec7-5ccd-4b01-bbfe-31c06d160f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
